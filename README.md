# Project-on-NLP-Tokenization

This project is done on Tokenization(NLP) to understand the concepts of it.

the query is taken from kaggle

Tokenize text using the tidytext package and calculate token frequency and find out if there's a relationship between how often different children use disfluencies (words like "um" or "uh") and how long they've been exposed to English.

I used a corpus of transcribed speech from bi-lingual children speaking in English.

This dataset of kid's speech is in a bit of a different file format. These files were generated by CLAN, a specialized program for transcribing children's speech. Under the hood, however, they're just text files with some additional formatting. With a little text processing we can just treat them like raw text files.

Tokens" are usually individual words  and "tokenization" is taking a text or set of text and breaking it up into individual its words. These tokens are then used as the input for other types of analysis or tasks, like parsing (automatically tagging the syntactic relationship between words).

